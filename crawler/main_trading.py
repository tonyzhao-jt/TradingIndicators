#!/usr/bin/env python3
"""
TradingViewè„šæœ¬çˆ¬è™« - ä¸»ç¨‹åº
åŸºäºSeleniumçš„å®Œæ•´TradingView Pine Scriptçˆ¬è™«ï¼Œæ”¯æŒå¤šé¡µé¢çˆ¬å–å’Œå®Œæ•´æºä»£ç æå–
"""
import sys
import os
import json
import argparse
import signal
import time
from datetime import datetime
from pathlib import Path

# è®¾ç½®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

from impl.trading_view_script_crawler import TradingViewScriptCrawler


def load_existing_output(path):
    """Load existing detailed output file and return mapping by script_url"""
    if not path:
        return {}
    if not os.path.exists(path):
        return {}
    try:
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            if isinstance(data, list):
                return {item.get('script_url'): item for item in data}
    except Exception:
        return {}
    return {}

class TimeoutError(Exception):
    """Timeout exception for long-running operations"""
    pass

def timeout_handler(signum, frame):
    """Signal handler for timeout"""
    raise TimeoutError("Operation timed out")

def crawl_with_timeout(func, timeout_seconds=120, *args, **kwargs):
    """
    Execute a function with a timeout. Skip if it takes longer than timeout_seconds.
    Default timeout is 120 seconds (2 minutes).
    """
    # Set up signal alarm for timeout
    signal.signal(signal.SIGALRM, timeout_handler)
    signal.alarm(timeout_seconds)
    
    try:
        result = func(*args, **kwargs)
        signal.alarm(0)  # Cancel alarm
        return result
    except TimeoutError:
        signal.alarm(0)  # Cancel alarm
        print(f"â° Timeout ({timeout_seconds}s) reached, skipping...")
        return None
    except Exception as e:
        signal.alarm(0)  # Cancel alarm
        print(f"âŒ Error during crawling: {e}")
        return None

def setup_argparser():
    """è®¾ç½®å‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(
        description='TradingView Pine Scriptçˆ¬è™« - å®Œæ•´æºä»£ç æå–å·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  %(prog)s --pages 5                          # çˆ¬å–å‰5é¡µï¼Œé»˜è®¤è¾“å‡ºè·¯å¾„
  %(prog)s --pages 10 --output ./results      # çˆ¬å–å‰10é¡µï¼ŒæŒ‡å®šè¾“å‡ºç›®å½•
  %(prog)s --pages 3 --max-per-page 10        # æ¯é¡µæœ€å¤š10ä¸ªè„šæœ¬
  %(prog)s --start-page 2 --pages 3           # ä»ç¬¬2é¡µå¼€å§‹ï¼Œçˆ¬å–3é¡µ
  %(prog)s --pages 1 --no-selenium           # ç¦ç”¨Seleniumï¼ˆä¸æ¨èï¼‰
        """
    )
    
    parser.add_argument(
        '--pages', 
        type=int, 
        default=1,
        help='è¦çˆ¬å–çš„é¡µé¢æ•°é‡ (é»˜è®¤: 1)'
    )
    
    parser.add_argument(
        '--start-page',
        type=int,
        default=1,
        help='èµ·å§‹é¡µé¢ç¼–å· (é»˜è®¤: 1)'
    )
    
    parser.add_argument(
        '--output', '-o',
        type=str,
        default='./output',
        help='è¾“å‡ºç›®å½•è·¯å¾„ (é»˜è®¤: ./output)'
    )

    parser.add_argument(
        '--output-file',
        type=str,
        default=None,
        help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œè‹¥æä¾›å°†å†™å…¥è¯¥æ–‡ä»¶ï¼ˆé»˜è®¤: <output>/multi_page_detailed_latest.jsonï¼‰'
    )
    
    parser.add_argument(
        '--max-per-page',
        type=int,
        default=None,
        help='æ¯é¡µæœ€å¤§è„šæœ¬æ•°é‡ï¼ŒNoneè¡¨ç¤ºä¸é™åˆ¶ (é»˜è®¤: None)'
    )
    
    parser.add_argument(
        '--base-url',
        type=str,
        default='https://www.tradingview.com/scripts/',
        help='TradingViewè„šæœ¬åˆ—è¡¨åŸºç¡€URL (é»˜è®¤: https://www.tradingview.com/scripts/)'
    )
    
    parser.add_argument(
        '--no-selenium',
        action='store_true',
        help='ç¦ç”¨Selenium JavaScriptæ¸²æŸ“ (ä¸æ¨èï¼Œä¼šå¯¼è‡´æºä»£ç æå–å¤±è´¥)'
    )
    
    parser.add_argument(
        '--preview-only',
        action='store_true',
        help='ä»…çˆ¬å–é¢„è§ˆä¿¡æ¯ï¼Œä¸è·å–è¯¦ç»†ä¿¡æ¯ (æ›´å¿«ä½†ä¿¡æ¯ä¸å®Œæ•´)'
    )
    
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='å¯ç”¨è¯¦ç»†æ—¥å¿—è¾“å‡º'
    )
    
    parser.add_argument(
        '--quiet', '-q',
        action='store_true',
        help='é™é»˜æ¨¡å¼ï¼Œæœ€å°åŒ–è¾“å‡º'
    )

    parser.add_argument(
        '--overwrite',
        action='store_true',
        help='å¦‚æœå·²å­˜åœ¨è¾“å‡ºæ–‡ä»¶ï¼Œæ˜¯å¦é‡æ–°çˆ¬å–å·²å­˜åœ¨çš„æ¡ç›®ï¼ˆé»˜è®¤: è·³è¿‡ç°æœ‰æ¡ç›®ï¼‰'
    )
    
    return parser

def validate_args(args):
    """éªŒè¯å‘½ä»¤è¡Œå‚æ•°"""
    errors = []
    
    if args.pages <= 0:
        errors.append("é¡µé¢æ•°é‡å¿…é¡»å¤§äº0")
    
    if args.start_page <= 0:
        errors.append("èµ·å§‹é¡µé¢ç¼–å·å¿…é¡»å¤§äº0")
    
    if args.max_per_page is not None and args.max_per_page <= 0:
        errors.append("æ¯é¡µæœ€å¤§è„šæœ¬æ•°é‡å¿…é¡»å¤§äº0")
    
    if args.quiet and args.verbose:
        errors.append("ä¸èƒ½åŒæ—¶ä½¿ç”¨ --quiet å’Œ --verbose é€‰é¡¹")
    
    return errors

def print_header(args):
    """æ‰“å°ç¨‹åºå¤´éƒ¨ä¿¡æ¯"""
    if args.quiet:
        return
        
    print("=" * 80)
    print("ğŸš€ TradingView Pine Script çˆ¬è™«")
    print("=" * 80)
    print(f"ğŸ“‹ çˆ¬å–é…ç½®:")
    print(f"   é¡µé¢èŒƒå›´: {args.start_page}-{args.start_page + args.pages - 1} (å…±{args.pages}é¡µ)")
    print(f"   åŸºç¡€URL: {args.base_url}")
    print(f"   è¾“å‡ºç›®å½•: {args.output}")
    print(f"   æ¯é¡µæœ€å¤§è„šæœ¬: {args.max_per_page or 'æ— é™åˆ¶'}")
    print(f"   ä½¿ç”¨Selenium: {'å¦' if args.no_selenium else 'æ˜¯'}")
    print(f"   è¯¦ç»†ä¿¡æ¯: {'å¦' if args.preview_only else 'æ˜¯'}")
    print()

def crawl_preview_data(crawler, args):
    """çˆ¬å–é¢„è§ˆæ•°æ®"""
    if not args.quiet:
        print("ğŸ” ç¬¬ä¸€æ­¥: çˆ¬å–åˆ—è¡¨é¡µé¢é¢„è§ˆä¿¡æ¯")
        print("-" * 60)
    
    all_preview_data = []
    
    for page_num in range(args.start_page, args.start_page + args.pages):
        if not args.quiet:
            print(f"\nğŸ“„ çˆ¬å–ç¬¬ {page_num} é¡µ...")
        
        # æ„é€ é¡µé¢URL
        if page_num == 1:
            page_url = args.base_url
        else:
            separator = '&' if '?' in args.base_url else '?'
            page_url = f"{args.base_url}{separator}page={page_num}"
        
        if args.verbose:
            print(f"   URL: {page_url}")
        
        try:
            # æå–é“¾æ¥å’Œé¢„è§ˆä¿¡æ¯
            preview_links = crawler.extract_links(page_url)
            
            if preview_links:
                # é™åˆ¶æ¯é¡µçš„è„šæœ¬æ•°é‡
                if args.max_per_page:
                    limited_links = preview_links[:args.max_per_page]
                else:
                    limited_links = preview_links
                
                # æ·»åŠ é¡µé¢æ ‡è®°
                for link_info in limited_links:
                    link_info['crawl_page'] = page_num
                
                all_preview_data.extend(limited_links)
                
                if not args.quiet:
                    print(f"   âœ… æˆåŠŸæå– {len(limited_links)} ä¸ªè„šæœ¬é¢„è§ˆä¿¡æ¯")
                
                if args.verbose:
                    for i, script_info in enumerate(limited_links[:3]):  # æ˜¾ç¤ºå‰3ä¸ª
                        print(f"     [{i+1}] {script_info.get('preview_title', 'N/A')}")
                        print(f"         ä½œè€…: {script_info.get('preview_author', 'N/A')}")
                        print(f"         ç‚¹èµ: {script_info.get('preview_likes_count', 0)}")
                    if len(limited_links) > 3:
                        print(f"     ... è¿˜æœ‰ {len(limited_links) - 3} ä¸ªè„šæœ¬")
                        
            else:
                if not args.quiet:
                    print(f"   âš  ç¬¬ {page_num} é¡µæœªæå–åˆ°ä»»ä½•è„šæœ¬")
                    
        except Exception as e:
            print(f"   âŒ ç¬¬ {page_num} é¡µçˆ¬å–å¤±è´¥: {e}")
            if args.verbose:
                import traceback
                traceback.print_exc()
    
    return all_preview_data

def crawl_detailed_data(crawler, preview_data, args):
    """çˆ¬å–è¯¦ç»†æ•°æ®"""
    if args.preview_only:
        return preview_data
    
    if not args.quiet:
        print(f"\nğŸ” ç¬¬äºŒæ­¥: çˆ¬å–è¯¦ç»†ä¿¡æ¯ ({'Selenium' if not args.no_selenium else 'HTTP'})")
        print("-" * 60)
    
    detailed_data = []
    total_scripts = len(preview_data)
    
    for i, preview_info in enumerate(preview_data):
        script_url = preview_info.get('script_url')
        if not script_url:
            continue
        
        if not args.quiet:
            title = preview_info.get('preview_title', 'N/A')
            print(f"\nğŸ“ [{i+1}/{total_scripts}] {title}")
            
        if args.verbose:
            print(f"   URL: {script_url}")
        
        try:
            # æå–è¯¦ç»†ä¿¡æ¯ï¼ˆä½¿ç”¨è¶…æ—¶æœºåˆ¶ï¼‰
            start_time = time.time()
            detailed_info = crawl_with_timeout(
                crawler.extract_detailed_data,
                timeout_seconds=120,  # 2åˆ†é’Ÿè¶…æ—¶
                script_url=script_url,
                use_selenium=not args.no_selenium
            )
            elapsed = time.time() - start_time
            
            if detailed_info:
                # åˆå¹¶é¢„è§ˆä¿¡æ¯å’Œè¯¦ç»†ä¿¡æ¯
                combined_info = {**preview_info, **detailed_info}
                detailed_data.append(combined_info)
                
                if not args.quiet:
                    print(f"   âœ… æˆåŠŸ ({elapsed:.1f}s)")
                
                if args.verbose:
                    print(f"     æ ‡é¢˜: {detailed_info.get('name', 'N/A')}")
                    print(f"     ä½œè€…: {detailed_info.get('user', {}).get('username', 'N/A')}")
                    print(f"     ç‚¹èµ: {detailed_info.get('likes_count', 0)}")
                    print(f"     æºä»£ç : {len(detailed_info.get('source_code', '') or '')} å­—ç¬¦")
            else:
                if not args.quiet:
                    print(f"   âŒ è¯¦ç»†ä¿¡æ¯æå–å¤±è´¥ ({elapsed:.1f}s)")
                    
        except Exception as e:
            print(f"   âŒ å¼‚å¸¸: {e}")
            if args.verbose:
                import traceback
                traceback.print_exc()
    
    return detailed_data

def save_results(data, args):
    """ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶"""
    if not args.quiet:
        print(f"\nğŸ’¾ ä¿å­˜ç»“æœ")
        print("-" * 60)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    # å¦‚æœæŒ‡å®šäº†å®Œæ•´è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œåˆ™ä½¿ç”¨è¯¥è·¯å¾„
    if args.output_file:
        filepath = Path(args.output_file)
        filepath.parent.mkdir(parents=True, exist_ok=True)
    else:
        output_dir = Path(args.output)
        output_dir.mkdir(parents=True, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        data_type = "preview" if args.preview_only else "detailed"
        filename = f"tradingview_{data_type}_{timestamp}.json"
        filepath = output_dir / filename
    
    # ä¿å­˜æ•°æ®
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    
    if not args.quiet:
        print(f"   âœ… æ•°æ®å·²ä¿å­˜: {filepath}")
        print(f"   ğŸ“Š æ€»è„šæœ¬æ•°: {len(data)}")
    
    return str(filepath)

def print_summary(data, args, filepath):
    """æ‰“å°æ€»ç»“æŠ¥å‘Š"""
    if args.quiet:
        # é™é»˜æ¨¡å¼åªè¾“å‡ºæ–‡ä»¶è·¯å¾„
        print(filepath)
        return
    
    print(f"\nğŸ“ˆ çˆ¬å–æ€»ç»“")
    print("-" * 60)
    print(f"âœ… çˆ¬å–å®Œæˆ")
    print(f"")
    print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡:")
    print(f"   çˆ¬å–é¡µæ•°: {args.pages}")
    print(f"   æ€»è„šæœ¬æ•°: {len(data)}")
    
    if not args.preview_only and data:
        # è®¡ç®—è´¨é‡æŒ‡æ ‡
        source_code_count = sum(1 for item in data if item.get('source_code'))
        user_info_count = sum(1 for item in data if item.get('user', {}).get('username'))
        
        print(f"")
        print(f"ğŸ” è´¨é‡åˆ†æ:")
        print(f"   æºä»£ç æå–æˆåŠŸç‡: {source_code_count}/{len(data)} ({source_code_count/len(data)*100:.1f}%)")
        print(f"   ç”¨æˆ·ä¿¡æ¯æå–æˆåŠŸç‡: {user_info_count}/{len(data)} ({user_info_count/len(data)*100:.1f}%)")
        
        # ç»Ÿè®¡æ€»ç‚¹èµæ•°
        total_likes = sum(item.get('likes_count', 0) or item.get('preview_likes_count', 0) for item in data)
        if total_likes > 0:
            print(f"   æ€»ç‚¹èµæ•°: {total_likes:,}")
    
    print(f"")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {filepath}")
    print(f"\n" + "=" * 80)

def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = setup_argparser()
    args = parser.parse_args()
    
    # éªŒè¯å‚æ•°
    validation_errors = validate_args(args)
    if validation_errors:
        print("âŒ å‚æ•°é”™è¯¯:")
        for error in validation_errors:
            print(f"   - {error}")
        sys.exit(1)
    
    # æ‰“å°å¤´éƒ¨ä¿¡æ¯
    print_header(args)
    
    try:
        # åˆå§‹åŒ–çˆ¬è™«
        if not args.quiet:
            print("ğŸ”§ åˆå§‹åŒ–çˆ¬è™«...")
        
        crawler = TradingViewScriptCrawler()
        
        # ç¡®å®šè¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå•ä¸€JSONæ–‡ä»¶ä¿å­˜è¯¦ç»†ç»“æœï¼‰
        if args.output_file:
            output_file = args.output_file
        else:
            # é»˜è®¤æ”¾åœ¨è¾“å‡ºç›®å½•ä¸‹çš„ multi_page_detailed_latest.json
            os.makedirs(args.output, exist_ok=True)
            output_file = os.path.join(args.output, 'multi_page_detailed_latest.json')

        # è¯»å–å·²æœ‰ç»“æœï¼ˆå¦‚æœæœ‰ï¼‰ï¼Œç”¨äºè·³è¿‡å·²æŠ“å–æ¡ç›®
        existing_map = load_existing_output(output_file)
        if not args.quiet:
            print(f"Loaded {len(existing_map)} existing detailed items from {output_file}")

        # ç¬¬ä¸€æ­¥: çˆ¬å–é¢„è§ˆæ•°æ®
        preview_data = crawl_preview_data(crawler, args)
        
        if not preview_data:
            print("âŒ æœªçˆ¬å–åˆ°ä»»ä½•é¢„è§ˆæ•°æ®")
            sys.exit(1)

        # ç¬¬äºŒæ­¥: éå†é¢„è§ˆå¹¶çˆ¬å–è¯¦ç»†æ•°æ®ï¼ˆè·³è¿‡å·²å­˜åœ¨ï¼Œé™¤é --overwriteï¼‰
        detailed_results = []
        # å¦‚æœå·²æœ‰æ•°æ®ä¸”ä¸è¦†ç›–ï¼Œå…ˆæŠŠå·²æœ‰æ•°æ®åŠ å…¥ç»“æœï¼ˆä¿æŒé¡ºåºå¯èƒ½ä¸åŒï¼‰
        if not args.overwrite and existing_map:
            # We'll add existing items as placeholders and skip crawling them
            # Note: existing_map values are dicts
            pass

        total = len(preview_data)
        for idx, preview in enumerate(preview_data):
            script_url = preview.get('script_url')
            if not script_url:
                continue

            # å¦‚æœå·²å­˜åœ¨å¹¶ä¸”ä¸è¦†ç›–ï¼Œç›´æ¥ä½¿ç”¨å·²æœ‰æ•°æ®
            if script_url in existing_map and not args.overwrite:
                if not args.quiet:
                    print(f"Skipping existing: {script_url}")
                detailed_results.append(existing_map[script_url])
                continue

            if not args.quiet:
                print(f"[{idx+1}/{total}] Crawling detail: {script_url}")

            # ä½¿ç”¨è¶…æ—¶æœºåˆ¶çˆ¬å–è¯¦ç»†æ•°æ®ï¼ˆ2åˆ†é’Ÿè¶…æ—¶ï¼‰
            start_time = time.time()
            detail = crawl_with_timeout(
                crawler.extract_detailed_data,
                timeout_seconds=120,  # 2åˆ†é’Ÿè¶…æ—¶
                script_url=script_url,
                use_selenium=not args.no_selenium
            )
            elapsed = time.time() - start_time
            
            if detail:
                if not args.quiet:
                    print(f"âœ… Extracted detail in {elapsed:.1f}s")
                merged = {**preview, **detail}
                detailed_results.append(merged)

                # å¢é‡ä¿å­˜åˆ°è¾“å‡ºæ–‡ä»¶ï¼Œé˜²æ­¢ä¸­é€”ä¸¢å¤±
                try:
                    with open(output_file, 'w', encoding='utf-8') as f:
                        json.dump(detailed_results, f, indent=2, ensure_ascii=False)
                except Exception as e:
                    print(f"Warning: incremental save failed: {e}")
            else:
                if not args.quiet:
                    print(f"âŒ Failed to extract detail for {script_url} (took {elapsed:.1f}s)")

        # ç¬¬ä¸‰æ­¥: æœ€ç»ˆä¿å­˜ï¼ˆè¦†ç›–æ–‡ä»¶ï¼‰
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(detailed_results, f, indent=2, ensure_ascii=False)
            filepath = output_file
        except Exception as e:
            print(f"Failed to write final output: {e}")
            filepath = None
        
        # ç¬¬å››æ­¥: æ‰“å°æ€»ç»“
        print_summary(detailed_results, args, filepath)
        
    except KeyboardInterrupt:
        print("\nâŒ ç”¨æˆ·ä¸­æ–­çˆ¬å–")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ çˆ¬å–è¿‡ç¨‹å‡ºç°å¼‚å¸¸: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()