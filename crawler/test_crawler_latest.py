#!/usr/bin/env python3
"""
æœ€æ–°çš„TradingViewçˆ¬è™«æµ‹è¯• - å¤šé¡µæ•°æ®æå–
æµ‹è¯•å®Œæ•´çš„çˆ¬å–æµç¨‹ï¼ŒåŒ…æ‹¬åˆ—è¡¨é¡µé¢é¢„è§ˆä¿¡æ¯å’Œè¯¦æƒ…é¡µé¢å®Œæ•´ä¿¡æ¯
"""
import sys
import os
import json
from datetime import datetime

# è®¾ç½®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

from impl.trading_view_script_crawler import TradingViewScriptCrawler

def test_multi_page_crawling():
    """æµ‹è¯•å¤šé¡µé¢çˆ¬å–åŠŸèƒ½"""
    
    print("=" * 80)
    print("TradingViewçˆ¬è™« - å¤šé¡µæ•°æ®æå–æµ‹è¯•")
    print("=" * 80)
    
    # åˆå§‹åŒ–çˆ¬è™«
    crawler = TradingViewScriptCrawler()
    
    # é…ç½®å‚æ•°
    base_url = "https://www.tradingview.com/scripts/"
    start_page = 1
    end_page = 2  # æµ‹è¯•å‰2é¡µ
    max_scripts_per_page = 5  # æ¯é¡µæœ€å¤š5ä¸ªè„šæœ¬
    
    print(f"ğŸ“‹ æµ‹è¯•é…ç½®:")
    print(f"   åŸºç¡€URL: {base_url}")
    print(f"   é¡µé¢èŒƒå›´: {start_page}-{end_page}")
    print(f"   æ¯é¡µæœ€å¤§è„šæœ¬æ•°: {max_scripts_per_page}")
    
    # 1. æµ‹è¯•åˆ—è¡¨é¡µé¢é¢„è§ˆä¿¡æ¯æå–
    print(f"\nğŸ” ç¬¬ä¸€æ­¥: æµ‹è¯•åˆ—è¡¨é¡µé¢é¢„è§ˆä¿¡æ¯æå–")
    print("-" * 60)
    
    all_preview_data = []
    
    try:
        for page_num in range(start_page, end_page + 1):
            print(f"\nğŸ“„ çˆ¬å–ç¬¬ {page_num} é¡µ...")
            
            # æ„é€ é¡µé¢URL
            page_url = f"{base_url}?page={page_num}" if page_num > 1 else base_url
            print(f"   URL: {page_url}")
            
            # æå–é“¾æ¥å’Œé¢„è§ˆä¿¡æ¯
            preview_links = crawler.extract_links(page_url)
            
            if preview_links:
                # é™åˆ¶æ¯é¡µçš„è„šæœ¬æ•°é‡
                limited_links = preview_links[:max_scripts_per_page]
                
                # æ·»åŠ é¡µé¢æ ‡è®°
                for link_info in limited_links:
                    link_info['crawl_page'] = page_num
                
                all_preview_data.extend(limited_links)
                
                print(f"   âœ… æˆåŠŸæå– {len(limited_links)} ä¸ªè„šæœ¬é¢„è§ˆä¿¡æ¯")
                
                # æ˜¾ç¤ºéƒ¨åˆ†é¢„è§ˆä¿¡æ¯
                for i, script_info in enumerate(limited_links[:3]):  # æ˜¾ç¤ºå‰3ä¸ª
                    print(f"     [{i+1}] {script_info.get('preview_title', 'N/A')}")
                    print(f"         ä½œè€…: {script_info.get('preview_author', 'N/A')}")
                    print(f"         ç‚¹èµ: {script_info.get('preview_likes_count', 0)}")
                    print(f"         è¯„è®º: {script_info.get('preview_comments_count', 0)}")
                if len(limited_links) > 3:
                    print(f"     ... è¿˜æœ‰ {len(limited_links) - 3} ä¸ªè„šæœ¬")
            else:
                print(f"   âŒ ç¬¬ {page_num} é¡µæœªæå–åˆ°ä»»ä½•è„šæœ¬")
        
        print(f"\nğŸ“Š é¢„è§ˆä¿¡æ¯æå–æ€»ç»“:")
        print(f"   æ€»è„šæœ¬æ•°: {len(all_preview_data)}")
        
        # ç»Ÿè®¡æ•°æ®
        total_likes = sum(item.get('preview_likes_count', 0) for item in all_preview_data)
        total_comments = sum(item.get('preview_comments_count', 0) for item in all_preview_data)
        print(f"   æ€»ç‚¹èµæ•°: {total_likes:,}")
        print(f"   æ€»è¯„è®ºæ•°: {total_comments:,}")
        
    except Exception as e:
        print(f"   âŒ é¢„è§ˆä¿¡æ¯æå–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # 2. æµ‹è¯•è¯¦æƒ…é¡µé¢ä¿¡æ¯æå– (ä½¿ç”¨Selenium)
    print(f"\nğŸ” ç¬¬äºŒæ­¥: æµ‹è¯•è¯¦æƒ…é¡µé¢å®Œæ•´ä¿¡æ¯æå– (Selenium)")
    print("-" * 60)
    
    detailed_data = []
    
    # é€‰æ‹©å‰3ä¸ªè„šæœ¬è¿›è¡Œè¯¦ç»†ä¿¡æ¯æå–
    test_scripts = all_preview_data[:3]
    
    for i, preview_info in enumerate(test_scripts):
        script_url = preview_info.get('script_url')
        if not script_url:
            continue
        
        print(f"\nğŸ“ [{i+1}/3] æå–è¯¦ç»†ä¿¡æ¯: {preview_info.get('preview_title', 'N/A')}")
        print(f"   URL: {script_url}")
        
        try:
            # æå–è¯¦ç»†ä¿¡æ¯
            detailed_info = crawler.extract_detailed_data(script_url)
            
            if detailed_info:
                # åˆå¹¶é¢„è§ˆä¿¡æ¯å’Œè¯¦ç»†ä¿¡æ¯
                combined_info = {**preview_info, **detailed_info}
                detailed_data.append(combined_info)
                
                print(f"   âœ… æˆåŠŸæå–è¯¦ç»†ä¿¡æ¯")
                print(f"     æ ‡é¢˜: {detailed_info.get('name', 'N/A')}")
                print(f"     ä½œè€…: {detailed_info.get('user', {}).get('username', 'N/A')}")
                print(f"     è¯¦æƒ…é¡µç‚¹èµæ•°: {detailed_info.get('likes_count', 0)}")
                print(f"     æºä»£ç é•¿åº¦: {len(detailed_info.get('source_code', '') or '')}")
                
                # æ¯”è¾ƒé¢„è§ˆå’Œè¯¦æƒ…çš„ç‚¹èµæ•°
                preview_likes = preview_info.get('preview_likes_count', 0)
                detail_likes = detailed_info.get('likes_count', 0)
                if preview_likes != detail_likes:
                    print(f"     âš  ç‚¹èµæ•°ä¸ä¸€è‡´: é¢„è§ˆ={preview_likes}, è¯¦æƒ…={detail_likes}")
                else:
                    print(f"     âœ… ç‚¹èµæ•°ä¸€è‡´: {preview_likes}")
            else:
                print(f"   âŒ è¯¦ç»†ä¿¡æ¯æå–å¤±è´¥")
                
        except Exception as e:
            print(f"   âŒ è¯¦ç»†ä¿¡æ¯æå–å¼‚å¸¸: {e}")
    
    # 3. ä¿å­˜ç»“æœ
    print(f"\nğŸ’¾ ç¬¬ä¸‰æ­¥: ä¿å­˜æµ‹è¯•ç»“æœ")
    print("-" * 60)
    
    # ä¿å­˜é¢„è§ˆæ•°æ®
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    preview_file = f"test_results/multi_page_preview_{timestamp}.json"
    os.makedirs("test_results", exist_ok=True)
    
    with open(preview_file, 'w', encoding='utf-8') as f:
        json.dump(all_preview_data, f, indent=2, ensure_ascii=False)
    print(f"   âœ… é¢„è§ˆæ•°æ®å·²ä¿å­˜: {preview_file}")
    
    # ä¿å­˜è¯¦ç»†æ•°æ®
    if detailed_data:
        detailed_file = f"test_results/multi_page_detailed_{timestamp}.json"
        with open(detailed_file, 'w', encoding='utf-8') as f:
            json.dump(detailed_data, f, indent=2, ensure_ascii=False)
        print(f"   âœ… è¯¦ç»†æ•°æ®å·²ä¿å­˜: {detailed_file}")
    
    # 4. æ€»ç»“æŠ¥å‘Š
    print(f"\nğŸ“ˆ ç¬¬å››æ­¥: æµ‹è¯•æ€»ç»“")
    print("-" * 60)
    
    print(f"âœ… æˆåŠŸå®Œæˆå¤šé¡µçˆ¬å–æµ‹è¯•")
    print(f"")
    print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡:")
    print(f"   çˆ¬å–é¡µæ•°: {end_page - start_page + 1}")
    print(f"   é¢„è§ˆè„šæœ¬æ€»æ•°: {len(all_preview_data)}")
    print(f"   è¯¦ç»†ä¿¡æ¯è„šæœ¬æ•°: {len(detailed_data)}")
    
    if detailed_data:
        print(f"")
        print(f"ğŸ” è´¨é‡åˆ†æ:")
        
        # æºä»£ç æå–æˆåŠŸç‡
        source_code_count = sum(1 for item in detailed_data if item.get('source_code'))
        print(f"   æºä»£ç æå–æˆåŠŸç‡: {source_code_count}/{len(detailed_data)} ({source_code_count/len(detailed_data)*100:.1f}%)")
        
        # ç”¨æˆ·ä¿¡æ¯æå–æˆåŠŸç‡
        user_info_count = sum(1 for item in detailed_data if item.get('user', {}).get('username'))
        print(f"   ç”¨æˆ·ä¿¡æ¯æå–æˆåŠŸç‡: {user_info_count}/{len(detailed_data)} ({user_info_count/len(detailed_data)*100:.1f}%)")
        
        # ç‚¹èµæ•°å¯¹æ¯”
        consistent_likes = sum(1 for item in detailed_data 
                             if item.get('preview_likes_count') == item.get('likes_count'))
        print(f"   ç‚¹èµæ•°ä¸€è‡´æ€§: {consistent_likes}/{len(detailed_data)} ({consistent_likes/len(detailed_data)*100:.1f}%)")
    
    print(f"\n" + "=" * 80)
    print("æµ‹è¯•å®Œæˆï¼")
    print("=" * 80)

if __name__ == "__main__":
    test_multi_page_crawling()