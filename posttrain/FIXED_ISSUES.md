# âœ… VERL è®­ç»ƒé…ç½®ä¿®å¤æ€»ç»“

## ğŸ”§ ä¿®å¤çš„é—®é¢˜

### 1. Hydra é…ç½®è¯­æ³•é”™è¯¯
**é—®é¢˜**: `reward_function.init_kwargs` JSON æ ¼å¼ä¸è¢« Hydra æ”¯æŒ
```bash
# âŒ é”™è¯¯é…ç½®
reward_function.init_kwargs='{"validation_file": "/path/to/val.parquet"}'
```

**è§£å†³æ–¹æ¡ˆ**: ä½¿ç”¨ VERL åŸç”Ÿçš„ `custom_reward_function` é…ç½®
```bash
# âœ… æ­£ç¡®é…ç½®  
custom_reward_function.path=/workspace/trading_indicators/posttrain/reward_plain.py
custom_reward_function.name=compute_score
```

### 2. Reward Function æ¥å£é€‚é…
**é—®é¢˜**: PlainRewardFunction ç±»ä¸ç¬¦åˆ VERL æœŸæœ›çš„å‡½æ•°æ¥å£

**è§£å†³æ–¹æ¡ˆ**: æ·»åŠ  VERL å…¼å®¹çš„ `compute_score` å‡½æ•°
```python
def compute_score(prompts, responses, **kwargs):
    """VERL-compatible reward computation function."""
    if not hasattr(compute_score, '_reward_fn'):
        compute_score._reward_fn = PlainRewardFunction()
    
    scores = []
    for prompt, response in zip(prompts, responses):
        score = compute_score._reward_fn(prompt, response)
        scores.append(score)
    
    return scores
```

### 3. è‡ªåŠ¨éªŒè¯æ•°æ®åŠ è½½
**é—®é¢˜**: æ— æ³•é€šè¿‡é…ç½®ä¼ é€’éªŒè¯æ•°æ®æ–‡ä»¶è·¯å¾„

**è§£å†³æ–¹æ¡ˆ**: åœ¨ PlainRewardFunction åˆå§‹åŒ–æ—¶è‡ªåŠ¨æ£€æµ‹å’ŒåŠ è½½éªŒè¯æ•°æ®
```python
# Auto-load validation data if available
if reference_data is None:
    validation_file = "/workspace/trading_indicators/outputs/data_splits/val.parquet"
    if os.path.exists(validation_file):
        self.load_reference_data(validation_file)
```

## ğŸ¯ æœ€ç»ˆå·¥ä½œé…ç½®

### è®­ç»ƒè„šæœ¬: `pt_verl_plain.sh`
```bash
python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=grpo \
    data.train_files=/workspace/trading_indicators/outputs/data_splits/train.parquet \
    data.val_files=/workspace/trading_indicators/outputs/data_splits/val.parquet \
    data.train_batch_size=8 \
    actor_rollout_ref.model.path=Qwen/Qwen3-Coder-30B-A3B-Instruct \
    custom_reward_function.path=/workspace/trading_indicators/posttrain/reward_plain.py \
    custom_reward_function.name=compute_score \
    trainer.total_epochs=50
```

### æ•°æ®é…ç½®
- **è®­ç»ƒæ ·æœ¬**: 25 æ¡
- **éªŒè¯æ ·æœ¬**: 7 æ¡  
- **æ•°æ®æ ¼å¼**: åŒ…å« `prompt`, `response`, `reward` åˆ—
- **æ¨¡å‹**: Qwen3-Coder-30B-A3B-Instruct

### Reward Function ç‰¹æ€§
- **ç›¸ä¼¼åº¦è¯„ä¼° (60%)**: LLM åˆ¤æ–­ä¸å‚è€ƒæ•°æ®çš„ç›¸ä¼¼åº¦
- **ä»£ç æ­£ç¡®æ€§ (40%)**: è¯­æ³•æ£€æŸ¥ + LLM ä»£ç è´¨é‡è¯„ä¼°
- **æ™ºèƒ½è¯„åˆ¤**: ä½¿ç”¨ Qwen3-Coder-30B è¿›è¡Œè¯­ä¹‰ç†è§£
- **å‚è€ƒå¯¹æ¯”**: ä¸éªŒè¯é›†æ•°æ®è¿›è¡Œæ™ºèƒ½æ¯”è¾ƒ

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### å¿«é€Ÿå¯åŠ¨
```bash
cd /workspace/trading_indicators/posttrain
./pt_verl_plain.sh
```

### å®Œæ•´è®¾ç½®æµç¨‹
```bash
cd /workspace/trading_indicators/posttrain  
./setup_plain_training.sh
```

### éªŒè¯é…ç½®
```bash
cd /workspace/trading_indicators/posttrain
./test_setup.sh
```

## ğŸ“Š è®­ç»ƒç›‘æ§

- **Wandb é¡¹ç›®**: `verl_trading_plain_reward`
- **å®éªŒå**: `qwen3_30b_plain_reward_v1`  
- **å…³é”®æŒ‡æ ‡**: 
  - `reward`: å¹³å‡å¥–åŠ±åˆ†æ•°
  - `policy_loss`: ç­–ç•¥ç½‘ç»œæŸå¤±
  - `value_loss`: ä»·å€¼ç½‘ç»œæŸå¤±

## ğŸ¯ é…ç½®ä¼˜åŒ–ç‚¹

### å†…å­˜ä¼˜åŒ–
```bash
actor_rollout_ref.actor.fsdp_config.param_offload=True      # å‚æ•°offload
actor_rollout_ref.actor.fsdp_config.optimizer_offload=True  # ä¼˜åŒ–å™¨offload  
actor_rollout_ref.rollout.gpu_memory_utilization=0.8       # 80% GPUå†…å­˜
```

### å¹¶è¡Œé…ç½®
```bash
actor_rollout_ref.rollout.tensor_model_parallel_size=4  # 4è·¯å¼ é‡å¹¶è¡Œ
actor_rollout_ref.rollout.n=2                          # 2ä¸ªå“åº”å€™é€‰
```

### è®­ç»ƒå‚æ•°
```bash
data.train_batch_size=8                     # æ‰¹æ¬¡å¤§å°
actor_rollout_ref.actor.ppo_mini_batch_size=4  # PPOå°æ‰¹æ¬¡
trainer.total_epochs=50                     # è®­ç»ƒè½®æ•°
trainer.save_freq=2                        # æ¯2è½®ä¿å­˜
trainer.test_freq=1                        # æ¯è½®æµ‹è¯•
```

## âœ… éªŒè¯ç»“æœ

æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼š
- âœ… æ•°æ®æ–‡ä»¶å­˜åœ¨ä¸”æ ¼å¼æ­£ç¡® (25 train + 7 val)
- âœ… Reward function æ­£å¸¸å·¥ä½œ 
- âœ… VERL é…ç½®è¯­æ³•æ­£ç¡®
- âœ… LLM è¿æ¥æ­£å¸¸ (Qwen3-Coder-30B)

**ç°åœ¨å¯ä»¥å¼€å§‹ VERL è®­ç»ƒäº†ï¼** ğŸ‰