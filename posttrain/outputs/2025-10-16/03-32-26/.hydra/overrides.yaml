- algorithm.adv_estimator=grpo
- data.train_files=/workspace/trading_indicators/outputs/data_splits/train.parquet
- data.val_files=/workspace/trading_indicators/outputs/data_splits/val.parquet
- data.train_batch_size=4
- data.max_prompt_length=600
- data.max_response_length=3200
- data.filter_overlong_prompts=False
- data.truncation=truncate
- actor_rollout_ref.model.path=Qwen/Qwen3-Coder-30B-A3B-Instruct
- actor_rollout_ref.actor.optim.lr=5e-6
- actor_rollout_ref.model.use_remove_padding=True
- actor_rollout_ref.actor.ppo_mini_batch_size=2
- actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=1
- actor_rollout_ref.actor.use_kl_loss=True
- actor_rollout_ref.actor.kl_loss_coef=0.01
- actor_rollout_ref.actor.kl_loss_type=low_var_kl
- actor_rollout_ref.actor.entropy_coeff=0.01
- actor_rollout_ref.model.enable_gradient_checkpointing=True
- actor_rollout_ref.actor.fsdp_config.param_offload=True
- actor_rollout_ref.actor.fsdp_config.optimizer_offload=True
- actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=1
- actor_rollout_ref.rollout.tensor_model_parallel_size=4
- actor_rollout_ref.rollout.name=vllm
- actor_rollout_ref.rollout.gpu_memory_utilization=0.7
- actor_rollout_ref.rollout.n=1
- actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=1
- actor_rollout_ref.ref.fsdp_config.param_offload=True
- algorithm.use_kl_in_reward=True
- custom_reward_function.path=/workspace/trading_indicators/posttrain/reward_plain.py
- custom_reward_function.name=compute_score
- trainer.critic_warmup=2
- trainer.logger=["console","wandb"]
- trainer.project_name=verl_trading_plain_reward
- trainer.experiment_name=qwen3_30b_plain_reward_v1
- trainer.n_gpus_per_node=1
- trainer.nnodes=1
- trainer.save_freq=2
- trainer.test_freq=1
- trainer.total_epochs=50
